{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6zVOMmkERc465VbMK1W7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/blog_notebooks/blob/main/bq_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BigQuery via Vertex Pipelines\n",
        "\n",
        "In this notebook, we will look at performing BigQuery operations within components of a Vertex Pipeline.\n",
        "\n",
        "We will use both [Google Cloud Components](https://cloud.google.com/vertex-ai/docs/pipelines/gcpc-list#bqml_components) and [lightweight Python](https://www.kubeflow.org/docs/components/pipelines/v2/components/lightweight-python-components/) components."
      ],
      "metadata": {
        "id": "yWPhMj4mQafk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install --upgrade kfp google-cloud-aiplatform google-cloud-bigquery google-cloud-pipeline-components"
      ],
      "metadata": {
        "id": "pZdZYigtQcIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate via Colab (if on Vertex Workbench, you're already authenticated)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "T7nIARtmR4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The 'location' gotcha\n",
        "\n",
        "It is common to list project locations such as `us-central1` and `europe-west4` when using Vertex AI. BigQuery datasets are less granular in their location specification, please check the BQ console if in doubt. For example, below we have `US` rather than `us-central1`. BigQuery will not find the dataset if this is incorrect."
      ],
      "metadata": {
        "id": "4mdThTiqiina"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment variables\n",
        "PROJECT_ID=\"<your-project-id>\"\n",
        "# Make sure the add the BQ dataset location, vs the project location\n",
        "LOCATION=\"US\"\n",
        "DATASET=\"<your-bq-dataset-name>\"\n",
        "BQ_DATASET=f\"{PROJECT_ID}.{DATASET}\"\n",
        "TABLE_A=f\"{BQ_DATASET}.table_a\"\n",
        "TABLE_B=f\"{BQ_DATASET}.table_b\"\n",
        "BUCKET_URI=\"gs://<your-bucket-name>\"\n",
        "PIPELINE_ROOT=BUCKET_URI"
      ],
      "metadata": {
        "id": "XJK5r2BWSCot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick check on BigQuery tables\n",
        "\n",
        "This [colab](https://colab.sandbox.google.com/notebooks/bigquery.ipynb#scrollTo=LMNA-vBHPyHz) is an excellent resource for examples of how to interact with BigQuery from a colab.\n",
        "\n",
        "In this example, we start with a simple table, `table_a`, filter `table_a` to create `table_b`, then create `view_c` from `table_b`, storing the view in Cloud Storage.\n",
        "\n",
        "### Create the table\n",
        "\n",
        "Uncomment and run the cell below if not done already"
      ],
      "metadata": {
        "id": "oQn7YnNZTmM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project <your-project-id>\n",
        "\n",
        "# CREATE TABLE <your-project-id>.<your-dataset>.table_a (\n",
        "#     id INT64,\n",
        "#     x INT64,\n",
        "#     y INT64,\n",
        "#     PARTITIONDATE DATE\n",
        "# );\n",
        "\n",
        "# INSERT INTO <your-project-id>.<your-dataset>.table_a (id, x, y, PARTITIONDATE)\n",
        "# VALUES\n",
        "#     (0, 2, 3, '2023-12-12'),\n",
        "#     (1, 1, 2, '2023-12-12'),\n",
        "#     (2, 3, 4, '2023-12-13'),\n",
        "#     (3, 4, 5, '2024-01-15');\n"
      ],
      "metadata": {
        "id": "H-HL3s16Tfz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vertex Pipelines\n",
        "\n",
        "We initialize the Vertex SDK and do the rest from inside a pipeline."
      ],
      "metadata": {
        "id": "e66K5gISVKc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform as vertex_ai\n",
        "\n",
        "vertex_ai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ],
      "metadata": {
        "id": "xnaRZrZFTyXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a custom lightweight component. This means we have to be specific about what the compoent does. Jump ahead first to see the GCP components designed specifically for BigQuery before having a look at this one.\n",
        "\n",
        "Here we leverage the kfp `dsl.component` decorator to make a lightweight Python component that includes the relevant libraries for BigQuery and Cloud Storage."
      ],
      "metadata": {
        "id": "JoF4YJQ0Vttt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(\n",
        "   packages_to_install=[\"google-cloud-bigquery\", \"google-cloud-storage\"],\n",
        "   base_image=\"python:3.9\",\n",
        ")\n",
        "\n",
        "def write_to_gcs(\n",
        "    view_name: str,\n",
        "    dataset: str,\n",
        "    project: str,\n",
        "    bucket: str,\n",
        "    blob_path: str,\n",
        "    output_gcs_uri: dsl.OutputPath(str)\n",
        "):\n",
        "   # Import libraries from inside the component\n",
        "   from google.cloud import bigquery\n",
        "   from google.cloud import storage\n",
        "\n",
        "   # Set up the BigQuery client and initialize to\n",
        "   # the correct dataset\n",
        "   client = bigquery.Client(project=project)\n",
        "   dataset = client.dataset(dataset)\n",
        "   view = dataset.table(view_name)\n",
        "\n",
        "   # Save view to Cloud Storage\n",
        "   def save_view(bucket_name, blob_path):\n",
        "       storage_client = storage.Client()\n",
        "       bucket = storage_client.get_bucket(bucket)\n",
        "       blob = bucket.get_blob(blob_path)\n",
        "       content = blob.download_as_string()\n",
        "       # Resulting URI in component output\n",
        "       storage_uri = f\"gs://{bucket}/{blob_path}\"\n",
        "       output_gcs_uri.set(storage_uri)"
      ],
      "metadata": {
        "id": "5GW2l_JuukxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `create_table...` and `create_view...` components below use the `BigqueryQueryJobOp` Google Cloud Pipeline Components.\n",
        "\n",
        "Here is the specification:\n",
        "\n",
        "```\n",
        "v1.bigquery.BigqueryQueryJobOp(\n",
        "  destination_table: dsl.Output[google.BQTable],\n",
        "  gcp_resources: dsl.OutputPath(str),\n",
        "  query: str = '',\n",
        "  location: str = 'us-central1',\n",
        "  query_parameters: list[str] = [],\n",
        "  job_configuration_query: dict[str, str] = {},\n",
        "  labels: dict[str, str] = {},\n",
        "  encryption_spec_key_name: str = '',\n",
        "  project: str = '{{$.pipeline_google_cloud_project_id}}')\n",
        "```\n",
        "\n",
        "Most of the parameters above are optional. In our example below, we will keep to `query`, `project` and `location`.\n",
        "\n",
        "The component automatically returns:\n",
        "\n",
        "```\n",
        "destination_table: dsl.Output[google.BQTable]\n",
        "```\n",
        "This is table where the query results should be stored. This property must be set for large results that exceed the maximum response size. For queries that produce anonymous (cached) results, this field will be populated by BigQuery.\n",
        "\n",
        "```\n",
        "gcp_resources: dsl.OutputPath(str)\n",
        "```\n",
        "Serialized gcp_resources proto tracking the BigQuery job. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md."
      ],
      "metadata": {
        "id": "_alC8j0hWKKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the query"
      ],
      "metadata": {
        "id": "1beqGJCchI7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's our SQL query to filter results from table_a to table_b\n",
        "\n",
        "query_a = f\"\"\"\n",
        "CREATE OR REPLACE TABLE {TABLE_B}\n",
        "AS SELECT id, x * y as xy FROM {TABLE_A}\n",
        "WHERE PARTITIONDATE = DATE(2023, 12, 12)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_ggNHAWnhMFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2 import dsl\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"bq-example\",\n",
        "    description=\"Showing bq components\",\n",
        ")\n",
        "def pipeline(\n",
        "    query: str = query_a,\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = LOCATION,\n",
        "    staging_dir: str = BUCKET_URI,\n",
        "\n",
        "):\n",
        "    # Import the libraries from inside the pipeline\n",
        "    from google_cloud_pipeline_components.types import artifact_types\n",
        "    from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
        "\n",
        "    create_table_b_op = BigqueryQueryJobOp(\n",
        "        query=query_a,\n",
        "        project=PROJECT_ID,\n",
        "        location=\"US\",\n",
        "    )\n",
        "\n",
        "    create_view_c_op = BigqueryQueryJobOp(\n",
        "        query=query_c,\n",
        "        project=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "    ).after(create_table_b_op)\n",
        "\n",
        "    save_view_op = write_to_gcs(\n",
        "        dataset=BQ_DATASET,\n",
        "        view_name=f\"{BQ_DATASET}.view_c\",\n",
        "        project=PROJECT_ID,\n",
        "        bucket=BUCKET_URI,\n",
        "        blob_path=f\"{BUCKET_URI}/view_c.csv\",\n",
        "    ).after(create_view_c_op)"
      ],
      "metadata": {
        "id": "P8PKyeCgWUfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2 import compiler\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"bq.json\")"
      ],
      "metadata": {
        "id": "v51SxzgOZD7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at the .json file with the pipeline parameters"
      ],
      "metadata": {
        "id": "3CTRyuFMaqMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat bq.json"
      ],
      "metadata": {
        "id": "tkvkJgfRXL8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Vertex AI Pipeline job\n",
        "job = vertex_ai.PipelineJob(\n",
        "    display_name='bq-example',\n",
        "    template_path='bq.json',\n",
        "    enable_caching=True,\n",
        ")\n",
        "# Run the pipeline job\n",
        "job.run()"
      ],
      "metadata": {
        "id": "PgYSerOm7Yxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/blog_notebooks/blob/main/images/bq_pipeline.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "FfbjA5HyfP-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should now see the `.csv` file in GCS with the `view_c` results.\n",
        "\n",
        "This can be the start of an end-to-end workflow, proceeding to model training, evaluation and deployment. For a more extensive example, please see this [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_bqml_pipeline_components.ipynb) from Google."
      ],
      "metadata": {
        "id": "rYeJwzvLhgQX"
      }
    }
  ]
}